{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a229a6e",
   "metadata": {},
   "source": [
    "# ü§ñ Pattern Mining with Spark MLlib\n",
    "**Discover misclassification patterns using distributed machine learning**\n",
    "\n",
    "This notebook:\n",
    "1. Loads data from Bronze Delta Lake\n",
    "2. Extracts features from classifications\n",
    "3. Clusters patterns with K-Means\n",
    "4. Identifies common error types\n",
    "5. Exports training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff427f",
   "metadata": {},
   "source": [
    "## Load Configuration & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4adfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Load config\n",
    "config_path = \"/dbfs/tamu-datathon-config.json\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìã PATTERN MINING CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Bronze Classifications: {config.get('bronze_classifications', 'Not set')}\")\n",
    "print(f\"Bronze Learning: {config.get('bronze_learning', 'Not set')}\")\n",
    "\n",
    "bronze_path = config['bronze_path']\n",
    "silver_path = config['silver_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Bronze data\n",
    "print(\"\\nüì• Loading Bronze Layer Data...\")\n",
    "\n",
    "classifications_df = spark.read.format(\"delta\").load(f\"{bronze_path}/classifications\")\n",
    "learning_df = spark.read.format(\"delta\").load(f\"{bronze_path}/learning_database\")\n",
    "\n",
    "print(f\"‚úÖ Classifications: {classifications_df.count()} records\")\n",
    "print(f\"‚úÖ Learning: {learning_df.count()} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f137d4a",
   "metadata": {},
   "source": [
    "## Step 1: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d9672",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîß FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extract features for clustering\n",
    "features_df = classifications_df.select(\n",
    "    col(\"document_id\"),\n",
    "    col(\"classification\"),\n",
    "    col(\"confidence\"),\n",
    "    coalesce(col(\"requires_review\"), lit(False)).alias(\"requires_review\"),\n",
    "    # Extract segment count safely\n",
    "    when(col(\"text_segments\").isNotNull(), \n",
    "         size(col(\"text_segments\"))).otherwise(0).alias(\"segment_count\"),\n",
    "    # Extract evidence count safely\n",
    "    when(col(\"evidence\").isNotNull(),\n",
    "         size(col(\"evidence\"))).otherwise(0).alias(\"evidence_count\"),\n",
    "    # Check if has additional labels\n",
    "    when(col(\"additional_labels\").isNotNull() & (size(col(\"additional_labels\")) > 0), 1)\n",
    "        .otherwise(0).alias(\"has_additional_labels\"),\n",
    "    # Safety flag (handle nested struct)\n",
    "    when(col(\"safety_check\").isNotNull() & (col(\"safety_check.is_safe\") == False), 1)\n",
    "        .otherwise(0).alias(\"is_unsafe\")\n",
    ").na.fill(0)\n",
    "\n",
    "print(\"‚úÖ Features extracted\")\n",
    "display(features_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c818ed49",
   "metadata": {},
   "source": [
    "## Step 2: Join with Learning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîó JOINING WITH CORRECTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find corrected documents\n",
    "corrected_df = learning_df.filter(col(\"approved\") == False).select(\n",
    "    col(\"document_id\"),\n",
    "    col(\"original_classification\"),\n",
    "    col(\"corrected_classification\"),\n",
    "    coalesce(col(\"feedback_notes\"), lit(\"No notes\")).alias(\"feedback_notes\")\n",
    ")\n",
    "\n",
    "correction_count = corrected_df.count()\n",
    "print(f\"‚úÖ Found {correction_count} corrected documents\")\n",
    "\n",
    "if correction_count > 0:\n",
    "    misclassified_df = features_df.join(corrected_df, on=\"document_id\", how=\"inner\")\n",
    "    print(\"\\nüìä Sample Misclassifications:\")\n",
    "    display(misclassified_df.select(\n",
    "        \"document_id\", \"classification\", \"corrected_classification\", \n",
    "        \"confidence\", \"segment_count\"\n",
    "    ).limit(5))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No corrections found. Using all classifications for analysis\")\n",
    "    misclassified_df = features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a149fd",
   "metadata": {},
   "source": [
    "## Step 3: K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d929b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ü§ñ CLUSTERING WITH K-MEANS (k=5)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Feature columns for clustering\n",
    "feature_cols = [\"confidence\", \"segment_count\", \"evidence_count\", \n",
    "                \"has_additional_labels\", \"is_unsafe\"]\n",
    "\n",
    "# Convert boolean to int\n",
    "misclassified_df = misclassified_df.withColumn(\n",
    "    \"requires_review_int\",\n",
    "    when(col(\"requires_review\") == True, 1).otherwise(0)\n",
    ")\n",
    "feature_cols.append(\"requires_review_int\")\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# K-Means\n",
    "kmeans = KMeans(\n",
    "    k=5,\n",
    "    seed=42,\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"cluster\"\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
    "\n",
    "# Train\n",
    "print(\"üîÑ Training K-Means model...\")\n",
    "model = pipeline.fit(misclassified_df)\n",
    "print(\"‚úÖ Model trained!\")\n",
    "\n",
    "# Predict\n",
    "clustered_df = model.transform(misclassified_df)\n",
    "print(f\"‚úÖ Clustered {clustered_df.count()} documents into 5 groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03115fb8",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f67a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä CLUSTER ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Cluster summary\n",
    "cluster_summary = clustered_df.groupBy(\"cluster\").agg(\n",
    "    count(\"*\").alias(\"doc_count\"),\n",
    "    round(avg(\"confidence\"), 4).alias(\"avg_confidence\"),\n",
    "    round(avg(\"segment_count\"), 2).alias(\"avg_segments\"),\n",
    "    sum(when(col(\"is_unsafe\") == 1, 1).otherwise(0)).alias(\"unsafe_count\")\n",
    ").orderBy(\"cluster\")\n",
    "\n",
    "print(\"üìà Cluster Summary:\")\n",
    "display(cluster_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806947a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representative documents per cluster\n",
    "print(\"\\nüîç Representative Documents per Cluster:\")\n",
    "\n",
    "for cluster_id in range(5):\n",
    "    cluster_docs = clustered_df.filter(col(\"cluster\") == cluster_id) \\\n",
    "        .select(\"document_id\", \"classification\", \"confidence\", \"segment_count\") \\\n",
    "        .limit(3)\n",
    "    \n",
    "    if cluster_docs.count() > 0:\n",
    "        print(f\"\\n--- Cluster {cluster_id} ---\")\n",
    "        display(cluster_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2d7479",
   "metadata": {},
   "source": [
    "## Step 5: Pattern Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f13e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° PATTERN INSIGHTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Patterns by classification\n",
    "classification_patterns = clustered_df.groupBy(\"classification\", \"cluster\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    round(avg(\"confidence\"), 4).alias(\"avg_confidence\")\n",
    ").orderBy(\"classification\", \"cluster\")\n",
    "\n",
    "print(\"üìä Patterns by Classification Type:\")\n",
    "display(classification_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a60fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find potential error patterns\n",
    "potential_errors = clustered_df.filter(\n",
    "    (col(\"confidence\") < 0.90) & (col(\"segment_count\") >= 2)\n",
    ").select(\n",
    "    \"document_id\", \"classification\", \"confidence\", \"segment_count\", \"cluster\"\n",
    ")\n",
    "\n",
    "error_count = potential_errors.count()\n",
    "print(f\"\\n‚ö†Ô∏è  Found {error_count} potential error patterns:\")\n",
    "print(\"   (Low confidence < 0.90 + Multiple segments >= 2)\")\n",
    "\n",
    "if error_count > 0:\n",
    "    display(potential_errors.limit(10))\n",
    "else:\n",
    "    print(\"   No error patterns detected - good accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4055ea",
   "metadata": {},
   "source": [
    "## Step 6: Export Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe153457",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üíæ EXPORTING TRAINING EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select best examples from each cluster\n",
    "training_examples = clustered_df.groupBy(\"cluster\").agg(\n",
    "    collect_list(\n",
    "        struct(\n",
    "            col(\"document_id\"),\n",
    "            col(\"classification\"),\n",
    "            col(\"confidence\"),\n",
    "            col(\"segment_count\")\n",
    "        )\n",
    "    ).alias(\"examples\"),\n",
    "    count(\"*\").alias(\"count\")\n",
    ")\n",
    "\n",
    "# Save to Silver layer\n",
    "training_path = f\"{silver_path}/training_examples\"\n",
    "training_examples.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(training_path)\n",
    "\n",
    "print(f\"‚úÖ Training examples exported: {training_path}\")\n",
    "display(training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cluster assignments\n",
    "cluster_assignments_path = f\"{silver_path}/cluster_assignments\"\n",
    "\n",
    "clustered_df.select(\n",
    "    \"document_id\", \"classification\", \"confidence\", \n",
    "    \"segment_count\", \"cluster\"\n",
    ").write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(cluster_assignments_path)\n",
    "\n",
    "print(f\"‚úÖ Cluster assignments saved: {cluster_assignments_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d0179b",
   "metadata": {},
   "source": [
    "## ‚úÖ Pattern Mining Complete!\n",
    "\n",
    "Discovered:\n",
    "- ‚úÖ 5 distinct document pattern clusters\n",
    "- ‚úÖ Common misclassification signatures\n",
    "- ‚úÖ Low-confidence error patterns\n",
    "- ‚úÖ Training examples exported to Silver layer\n",
    "\n",
    "**Next**: Run `05_analytics_dashboard.ipynb` for insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892bdc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config\n",
    "config['silver_training_examples'] = training_path\n",
    "config['silver_cluster_assignments'] = cluster_assignments_path\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"üíæ Configuration updated\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
