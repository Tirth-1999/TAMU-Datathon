{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c36c26",
   "metadata": {},
   "source": [
    "# ü•â Bronze Layer: Data Ingestion\n",
    "**Load classification results and learning database into Delta Lake**\n",
    "\n",
    "This notebook:\n",
    "1. Loads configuration from setup notebook\n",
    "2. Ingests classification JSON files ‚Üí Delta Lake\n",
    "3. Migrates learning_database.json ‚Üí Delta Lake\n",
    "4. Enables time travel and ACID transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1968a696",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeafad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Load config from setup notebook\n",
    "config_path = \"/dbfs/tamu-datathon-config.json\"\n",
    "if not os.path.exists(config_path):\n",
    "    raise Exception(\"‚ùå Configuration not found! Run 00_setup_and_verify.ipynb first\")\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìã LOADED CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Repository: {config['repo_path']}\")\n",
    "print(f\"Results: {config['results_path']}\")\n",
    "print(f\"Classification files: {config['num_classification_files']}\")\n",
    "print(f\"Learning entries: {config['num_learning_entries']}\")\n",
    "print(f\"Delta base: {config['delta_base']}\")\n",
    "\n",
    "# Extract paths\n",
    "results_path = config['results_path']\n",
    "learning_db_path = config['learning_db_path']\n",
    "bronze_path = config['bronze_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98e313b",
   "metadata": {},
   "source": [
    "## Step 1: Ingest Classification Results (Bronze Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac98ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üì• INGESTING CLASSIFICATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # List all JSON files\n",
    "    json_files = [f for f in os.listdir(results_path) \n",
    "                  if f.endswith('.json') and f != 'learning_database.json']\n",
    "    \n",
    "    if len(json_files) == 0:\n",
    "        print(\"‚ö†Ô∏è  No classification files found. Creating sample data...\")\n",
    "        sample_data = [{\n",
    "            \"document_id\": \"sample_001\",\n",
    "            \"filename\": \"sample_document.pdf\",\n",
    "            \"classification\": \"Public\",\n",
    "            \"confidence\": 0.95,\n",
    "            \"additional_labels\": [\"General Content\"],\n",
    "            \"requires_review\": False,\n",
    "            \"safety_check\": {\"is_safe\": True}\n",
    "        }]\n",
    "        classifications_df = spark.createDataFrame(sample_data)\n",
    "    else:\n",
    "        # Read all JSON files\n",
    "        all_data = []\n",
    "        for json_file in json_files:\n",
    "            file_path = os.path.join(results_path, json_file)\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                all_data.append(data)\n",
    "        \n",
    "        classifications_df = spark.createDataFrame(all_data)\n",
    "        print(f\"‚úÖ Loaded {len(all_data)} classification records\")\n",
    "    \n",
    "    # Write to Delta Lake\n",
    "    bronze_classifications_path = f\"{bronze_path}/classifications\"\n",
    "    classifications_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .save(bronze_classifications_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Bronze layer created: {bronze_classifications_path}\")\n",
    "    \n",
    "    # Verify\n",
    "    bronze_df = spark.read.format(\"delta\").load(bronze_classifications_path)\n",
    "    print(f\"‚úÖ Verified: {bronze_df.count()} records in Delta Lake\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac76d22",
   "metadata": {},
   "source": [
    "## Step 2: Ingest Learning Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b511a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìö INGESTING LEARNING DATABASE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    if os.path.exists(learning_db_path):\n",
    "        with open(learning_db_path, 'r') as f:\n",
    "            learning_raw = json.load(f)\n",
    "        \n",
    "        learning_data = learning_raw.get('learning_entries', [])\n",
    "        \n",
    "        if len(learning_data) > 0:\n",
    "            learning_df = spark.createDataFrame(learning_data)\n",
    "            print(f\"‚úÖ Loaded {len(learning_data)} learning records\")\n",
    "        else:\n",
    "            # Create sample\n",
    "            sample_learning = [{\n",
    "                \"document_id\": \"sample_001\",\n",
    "                \"original_classification\": \"Public\",\n",
    "                \"corrected_classification\": \"Confidential\",\n",
    "                \"approved\": False,\n",
    "                \"feedback_notes\": \"Contains internal business information\",\n",
    "                \"timestamp\": \"2024-11-09T10:00:00\"\n",
    "            }]\n",
    "            learning_df = spark.createDataFrame(sample_learning)\n",
    "            print(\"‚úÖ Created sample learning data\")\n",
    "    else:\n",
    "        sample_learning = [{\n",
    "            \"document_id\": \"sample_001\",\n",
    "            \"original_classification\": \"Public\",\n",
    "            \"corrected_classification\": \"Confidential\",\n",
    "            \"approved\": False,\n",
    "            \"feedback_notes\": \"Contains internal business information\"\n",
    "        }]\n",
    "        learning_df = spark.createDataFrame(sample_learning)\n",
    "        print(\"‚úÖ Created sample learning data\")\n",
    "    \n",
    "    # Write to Delta Lake\n",
    "    bronze_learning_path = f\"{bronze_path}/learning_database\"\n",
    "    learning_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .save(bronze_learning_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Learning database ingested: {bronze_learning_path}\")\n",
    "    \n",
    "    # Optimize\n",
    "    spark.sql(f\"OPTIMIZE delta.`{bronze_learning_path}`\")\n",
    "    print(\"‚úÖ Optimized Delta Lake storage\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1615f07",
   "metadata": {},
   "source": [
    "## Step 3: Verify Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff066fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ BRONZE LAYER VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Count records\n",
    "classifications_count = spark.read.format(\"delta\").load(f\"{bronze_path}/classifications\").count()\n",
    "learning_count = spark.read.format(\"delta\").load(f\"{bronze_path}/learning_database\").count()\n",
    "\n",
    "print(f\"üìä Bronze Layer Summary:\")\n",
    "print(f\"   Classifications: {classifications_count} records\")\n",
    "print(f\"   Learning Database: {learning_count} records\")\n",
    "\n",
    "# Show samples\n",
    "print(\"\\nüîç Sample Classification Record:\")\n",
    "display(spark.read.format(\"delta\").load(f\"{bronze_path}/classifications\").limit(1))\n",
    "\n",
    "print(\"\\nüîç Sample Learning Record:\")\n",
    "display(spark.read.format(\"delta\").load(f\"{bronze_path}/learning_database\").limit(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333591b7",
   "metadata": {},
   "source": [
    "## Step 4: Enable Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53396b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚è∞ DELTA LAKE HISTORY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show history\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY delta.`{bronze_path}/learning_database`\")\n",
    "display(history_df.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\"))\n",
    "\n",
    "print(\"\\n‚úÖ Time Travel enabled!\")\n",
    "print(f\"   Query past versions: SELECT * FROM delta.`{bronze_path}/learning_database` VERSION AS OF 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9867ab0",
   "metadata": {},
   "source": [
    "## ‚úÖ Ingestion Complete!\n",
    "\n",
    "Bronze layer created with:\n",
    "- ‚úÖ Classification results in Delta Lake\n",
    "- ‚úÖ Learning database in Delta Lake  \n",
    "- ‚úÖ Time Travel enabled\n",
    "- ‚úÖ ACID transactions enabled\n",
    "\n",
    "**Next**: Run `03_pattern_mining.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config\n",
    "config['bronze_classifications'] = f\"{bronze_path}/classifications\"\n",
    "config['bronze_learning'] = f\"{bronze_path}/learning_database\"\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"üíæ Configuration updated for next notebooks\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
