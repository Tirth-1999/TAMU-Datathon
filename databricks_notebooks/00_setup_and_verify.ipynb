{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c1f947",
   "metadata": {},
   "source": [
    "# üîç Setup & Data Verification\n",
    "**Automatically detect your repo path and verify data availability**\n",
    "\n",
    "Run this notebook FIRST to:\n",
    "1. Find your cloned repository\n",
    "2. Verify backend/results data exists\n",
    "3. Set up all necessary paths\n",
    "4. Create Delta Lake directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ced76d0",
   "metadata": {},
   "source": [
    "## Step 1: Auto-Detect Repository Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c1c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîç AUTO-DETECTING TAMU-DATATHON REPOSITORY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Try multiple possible locations\n",
    "possible_paths = [\n",
    "    \"/Workspace/Repos\",\n",
    "    \"/Repos\",\n",
    "    \"/Workspace/Users\"\n",
    "]\n",
    "\n",
    "repo_path = None\n",
    "backend_path = None\n",
    "\n",
    "for base in possible_paths:\n",
    "    try:\n",
    "        if os.path.exists(base):\n",
    "            for user_dir in os.listdir(base):\n",
    "                user_path = os.path.join(base, user_dir)\n",
    "                if os.path.isdir(user_path):\n",
    "                    for repo in os.listdir(user_path):\n",
    "                        if \"TAMU\" in repo.upper() or \"datathon\" in repo.lower():\n",
    "                            potential_repo = os.path.join(user_path, repo)\n",
    "                            backend_check = os.path.join(potential_repo, \"backend\")\n",
    "                            if os.path.exists(backend_check):\n",
    "                                repo_path = potential_repo\n",
    "                                backend_path = backend_check\n",
    "                                break\n",
    "                if repo_path:\n",
    "                    break\n",
    "        if repo_path:\n",
    "            break\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "if repo_path:\n",
    "    print(f\"‚úÖ FOUND REPOSITORY!\")\n",
    "    print(f\"   Repository: {repo_path}\")\n",
    "    print(f\"   Backend: {backend_path}\")\n",
    "else:\n",
    "    print(\"‚ùå Repository not found. Please provide manual path.\")\n",
    "    raise Exception(\"Repository not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b454aaae",
   "metadata": {},
   "source": [
    "## Step 2: Verify Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f41678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results directory\n",
    "results_path = os.path.join(backend_path, \"results\")\n",
    "learning_db_path = os.path.join(results_path, \"learning_database.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìÅ VERIFYING DATA FILES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Count JSON files in results\n",
    "json_files = []\n",
    "if os.path.exists(results_path):\n",
    "    for file in os.listdir(results_path):\n",
    "        if file.endswith('.json') and file != 'learning_database.json':\n",
    "            json_files.append(file)\n",
    "    \n",
    "    print(f\"‚úÖ Results directory: {results_path}\")\n",
    "    print(f\"   Classification files: {len(json_files)}\")\n",
    "    \n",
    "    if len(json_files) > 0:\n",
    "        sample_file = os.path.join(results_path, json_files[0])\n",
    "        with open(sample_file, 'r') as f:\n",
    "            sample_data = json.load(f)\n",
    "        print(f\"   Sample file: {json_files[0]}\")\n",
    "        print(f\"   Sample keys: {list(sample_data.keys())[:5]}\")\n",
    "else:\n",
    "    print(f\"‚ùå Results directory not found: {results_path}\")\n",
    "\n",
    "# Check learning database\n",
    "learning_data = []\n",
    "if os.path.exists(learning_db_path):\n",
    "    with open(learning_db_path, 'r') as f:\n",
    "        learning_raw = json.load(f)\n",
    "    # Extract learning_entries array\n",
    "    learning_data = learning_raw.get('learning_entries', [])\n",
    "    print(f\"\\n‚úÖ Learning database: {learning_db_path}\")\n",
    "    print(f\"   Total entries: {len(learning_data)}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Learning database not found: {learning_db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3775b1a6",
   "metadata": {},
   "source": [
    "## Step 3: Create Delta Lake Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e863d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üèóÔ∏è  CREATING DELTA LAKE STRUCTURE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use workspace-local path (not /mnt/ which is now restricted)\n",
    "# Option 1: Use workspace temp storage (works in all Databricks workspaces)\n",
    "DELTA_BASE = \"/tmp/tamu-datathon-delta\"\n",
    "\n",
    "# Option 2: If you have Unity Catalog, you can use Volumes instead:\n",
    "# DELTA_BASE = \"/Volumes/catalog_name/schema_name/volume_name/tamu-datathon-delta\"\n",
    "\n",
    "BRONZE_PATH = f\"{DELTA_BASE}/bronze\"\n",
    "SILVER_PATH = f\"{DELTA_BASE}/silver\"\n",
    "GOLD_PATH = f\"{DELTA_BASE}/gold\"\n",
    "\n",
    "# Create directories using dbutils (Databricks native way)\n",
    "try:\n",
    "    for path in [BRONZE_PATH, SILVER_PATH, GOLD_PATH]:\n",
    "        dbutils.fs.mkdirs(path)\n",
    "        print(f\"‚úÖ Created: {path}\")\n",
    "    print(f\"\\n‚úÖ Delta Lake structure ready at: {DELTA_BASE}\")\n",
    "except Exception as e:\n",
    "    # Fallback to local filesystem if dbutils fails\n",
    "    print(f\"‚ö†Ô∏è  dbutils.fs.mkdirs failed, using local filesystem\")\n",
    "    for path in [BRONZE_PATH, SILVER_PATH, GOLD_PATH]:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        print(f\"‚úÖ Created (local): {path}\")\n",
    "    print(f\"\\n‚úÖ Delta Lake structure ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb60c4",
   "metadata": {},
   "source": [
    "## Step 4: Save Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ff6a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save paths for other notebooks\n",
    "config = {\n",
    "    \"repo_path\": repo_path,\n",
    "    \"backend_path\": backend_path,\n",
    "    \"results_path\": results_path,\n",
    "    \"learning_db_path\": learning_db_path,\n",
    "    \"delta_base\": DELTA_BASE,\n",
    "    \"bronze_path\": BRONZE_PATH,\n",
    "    \"silver_path\": SILVER_PATH,\n",
    "    \"gold_path\": GOLD_PATH,\n",
    "    \"num_classification_files\": len(json_files),\n",
    "    \"num_learning_entries\": len(learning_data)\n",
    "}\n",
    "\n",
    "# Save to workspace-local temp storage (accessible in all Databricks workspaces)\n",
    "config_path = \"/tmp/tamu-datathon-config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üíæ CONFIGURATION SAVED\")\n",
    "print(\"=\" * 80)\n",
    "print(json.dumps(config, indent=2))\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete! Run the next notebooks in order:\")\n",
    "print(\"   1. 01_data_ingestion.ipynb\")\n",
    "print(\"   2. 03_pattern_mining.ipynb\") \n",
    "print(\"   3. 05_analytics_dashboard.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
